{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4TVOxVEaRBNK",
    "outputId": "dae3e13d-8f13-4ab3-f424-9ff47524cf4c",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.493437415Z",
     "start_time": "2023-11-10T10:36:19.447956297Z"
    }
   },
   "outputs": [],
   "source": [
    "# !unzip '/home/deepakachu/Desktop/DMS/project/deutschl.zip' -d '/home/deepakachu/Desktop/DMS/project/content'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keZH5uykd_zy"
   },
   "source": [
    "# Preprocessing of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "-cslrMnqVfCG",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.508544843Z",
     "start_time": "2023-11-10T10:36:19.458427750Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import music21 as m21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bao1FLoBD5G"
   },
   "source": [
    "## Converting songs to m21 score objects\n",
    " We filter out only the \".krn\" files from the dataset and parse the files to m21 converter that turns them into a m21 score object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "WwRArcJzV_Kq",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.558908597Z",
     "start_time": "2023-11-10T10:36:19.462092741Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_music(datapath):\n",
    "  songs = []\n",
    "  for path, subdirs, files in os.walk(datapath):\n",
    "    for file in files:\n",
    "      if file[-3:] == \"krn\":\n",
    "        song = m21.converter.parse(os.path.join(path, file))\n",
    "        songs.append(song)\n",
    "  return songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3Buht-FHGEe"
   },
   "source": [
    " We also filter out the songs by the duration of the notes that the song has. We want each not in the song to to be atleast a 16th note all the way to a while note. This is important since we want to convert the song to time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "urdx-xcZdc-B",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.560377106Z",
     "start_time": "2023-11-10T10:36:19.468049218Z"
    }
   },
   "outputs": [],
   "source": [
    "def acceptable(song):\n",
    "  acceptable_durations = [0.25, 0.5, 0.75, 1, 1.5, 2, 3, 4]\n",
    "  for note in song.flat.notesAndRests:\n",
    "    # \".flat\" flattens the song into a list of objects and \".notesAndRests\" filters out\n",
    "    # any other object that is not a note or a rest.\n",
    "    if note.duration.quarterLength not in acceptable_durations:\n",
    "      return False\n",
    "  return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Transposing the song to C major/ A minor\n",
    "We get the key from the score object, check if  the key is an instance of existing collection of keys, else estimate the key of the song by using the \"analyze\" function.\n",
    "\n",
    "We then find the interval for transposition. Here we will be transposing songs with major keys to \"C major\" and song with minor keys to \"A minor\".\n",
    "This simplification is so that the model has a more streamlined dataset and has to learn only 2 keys, reduce computational intensity and the size if the dataset.\n",
    "\n",
    "We calculate the interval between the song's key and the key we wish to transpose the song to using the \"Interval\" function of interval object.\n",
    "\n",
    "The function takes 2 pitch objects and returns the interval between them as an interval object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.567593433Z",
     "start_time": "2023-11-10T10:36:19.473211310Z"
    }
   },
   "outputs": [],
   "source": [
    "def transpose(song):\n",
    "  # get key of the song\n",
    "  parts = song.getElementsByClass(m21.stream.Part)\n",
    "  measures = parts[0].getElementsByClass(m21.stream.Measure)\n",
    "  key = measures[0][4]\n",
    "\n",
    "  # if not, estimate the key\n",
    "  if not isinstance(key, m21.key.Key):\n",
    "    key = song.analyze(\"key\")\n",
    "  print(key)\n",
    "\n",
    "  # calculate the interval for transposition\n",
    "  if key.mode == \"major\":\n",
    "    interval = m21.interval.Interval(key.tonic, m21.pitch.Pitch(\"D\"))\n",
    "  elif key.mode == \"minor\":\n",
    "    interval = m21.interval.Interval(key.tonic, m21.pitch.Pitch(\"G\"))\n",
    "\n",
    "  # transpose the song\n",
    "  transposed_song = song.transpose(interval)\n",
    "  return transposed_song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Encoding the song into time series data\n",
    "We analyse each note in the song and create a time series representation of the song.\n",
    "\n",
    "eg: C4 is mapped to 60 in MIDI.\n",
    "Suppose we have a C4 whole note (4 beats), we represent it as \\[ \"60\", \"\\_\", \"\\_\", \"\\_\"] where \"\\_\" represents time\\_step / 1 beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.570006999Z",
     "start_time": "2023-11-10T10:36:19.474500649Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_song(song, time_step = 0.25):\n",
    "  encoded_song = []\n",
    "  for event in song.flat.notesAndRests:\n",
    "    # check if the event is a note\n",
    "    if isinstance(event, m21.note.Note):\n",
    "      # store the MIDI equivalent of the note\n",
    "      symbol = event.pitch.midi\n",
    "    elif isinstance(event, m21.note.Rest):\n",
    "      symbol = \"r\"\n",
    "    # \".duration.quarterLength\" returns the beat of a note for which 4 beats = 1 note\n",
    "    steps = int(event.duration.quarterLength / time_step)\n",
    "    # for each note, encode the MIDI equivalent and the duration info.\n",
    "    for step in range(steps):\n",
    "      if step == 0:\n",
    "        encoded_song.append(symbol)\n",
    "      else:\n",
    "        encoded_song.append(\"_\")\n",
    "  # convert the encoded time series data to a string\n",
    "  encoded_song = \" \".join(map(str, encoded_song))\n",
    "  return encoded_song"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1GvSpG8PJr2"
   },
   "source": [
    "## The preprocess function\n",
    "\n",
    "We load the dataset, check if all the notes are of acceptable duration, transpose the song, encode it and then save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "b204oZCbdDq_",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.572259833Z",
     "start_time": "2023-11-10T10:36:19.481605396Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(datapath):\n",
    "  print(\"Loading songs...\")\n",
    "  songs = load_music(datapath)\n",
    "  print(f\"Loaded {len(songs)} songs.\")\n",
    "\n",
    "  for i, song in enumerate(songs):\n",
    "    if not acceptable(song):\n",
    "      continue\n",
    "\n",
    "    song = transpose(song)\n",
    "    encoded_song = encode_song(song)\n",
    "    save_path = os.path.join('/home/deepakachu/Desktop/DMS/project/content/dataset1', str(i))\n",
    "    with open(save_path, \"w\") as fp:\n",
    "      fp.write(encoded_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olZtxNghQ0N1"
   },
   "source": [
    "## Merging all the processed songs into a sequence\n",
    "We merge all the converted songs to a single file to facilitate the training of the NN, it is easier to manipulate/encode this single file to feed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "pN9UJuPNudyl",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.573438987Z",
     "start_time": "2023-11-10T10:36:19.484452333Z"
    }
   },
   "outputs": [],
   "source": [
    "def load(path):\n",
    "  # function to load a processed song given the path\n",
    "  with open(path, \"r\") as fp:\n",
    "    song = fp.read()\n",
    "  return song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "QJQs2smSskqx",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.576720315Z",
     "start_time": "2023-11-10T10:36:19.487071692Z"
    }
   },
   "outputs": [],
   "source": [
    "def single_file(processed_dataset_path, destination, seq_len = 64):\n",
    "  # load encoded songs and merge them using a delimiter, this function\n",
    "  # will create a sequence of 64 songs seperated by \"/\"\n",
    "  delimiter = \"/ \"*seq_len\n",
    "  songs = \"\"\n",
    "  for path, _, files in os.walk(processed_dataset_path):\n",
    "    for file in files:\n",
    "      file_path = os.path.join(path, file)\n",
    "      song = load(file_path)\n",
    "      songs = songs + song + \" \" + delimiter\n",
    "  songs = songs[:-1]\n",
    "\n",
    "  # write the merged sequence to the destination\n",
    "  with open(destination, \"w\") as fp:\n",
    "    fp.write(songs)\n",
    "  return songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XqOSgzhTo8N"
   },
   "source": [
    "## Map the merged sequence onto integers\n",
    "We need to convert the song sequence we have to an integer sequence in ordder to feedit to the model, so we need an integer mapping for all the symbols in the songs sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "UhXu3LNjf3n-",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.584610967Z",
     "start_time": "2023-11-10T10:36:19.490084284Z"
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "-WzpoTkue1cq",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.596292187Z",
     "start_time": "2023-11-10T10:36:19.534046809Z"
    }
   },
   "outputs": [],
   "source": [
    "def mapping(songs, mapping_path=\"/home/deepakachu/Desktop/DMS/project/content/dataset1/mapping.json\"):\n",
    "  # split the sequence, extract vocabulary\n",
    "  mappings = {}\n",
    "  songs = songs.split()\n",
    "  vocab = list(set(songs))\n",
    "\n",
    "  # map each symbol in vocabulary to a unique number\n",
    "  for i, symbol in enumerate(vocab):\n",
    "    mappings[symbol] = i\n",
    "\n",
    "  # save as a json file\n",
    "  with open(mapping_path, \"w\") as fp:\n",
    "    json.dump(mappings, fp, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9jYO5GfWY9t"
   },
   "source": [
    "## Converting the songs sequence to an integer sequence\n",
    "We use the integer mapping we obtained earlier to convert the songs sequence to an integer sequence by replacing the symbols with thier mapped integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "lVxcYY-iiGSM",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.596526657Z",
     "start_time": "2023-11-10T10:36:19.534316753Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_int(songs):\n",
    "  int_songs = []\n",
    "  # load the mapping json file\n",
    "  with open(\"/home/deepakachu/Desktop/DMS/project/content/dataset1/mapping.json\", \"r\") as fp:\n",
    "    mappings = json.load(fp)\n",
    "  # replace the symbols with their corresponding integer equivalent\n",
    "  songs = songs.split()\n",
    "  for symbol in songs:\n",
    "    int_songs.append(mappings[symbol])\n",
    "\n",
    "  return int_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNShn_cipNGN"
   },
   "source": [
    "# Generation of Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Q6-CxRH7pPOf",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.596605995Z",
     "start_time": "2023-11-10T10:36:19.534432813Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heAJnYPeai5D"
   },
   "source": [
    "## Generating training sequences\n",
    "Here we create the training sequences by generating sequences from the integer sequence, each sequence will have 64 elements that will be used as the historical data based on which the next element will be predicted by the model.\n",
    "\n",
    "The sequence length can also be altered so as to facilitate generation of more complex music.\n",
    "\n",
    "The sequences generated are then one-hot encoded to feed into the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "hEvVOfB7jqri",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.596677445Z",
     "start_time": "2023-11-10T10:36:19.534530088Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_train(seq_len=64):\n",
    "  songs = load(\"/home/deepakachu/Desktop/DMS/project/content/dataset1/single_file\")\n",
    "  int_songs = convert_to_int(songs)\n",
    "\n",
    "  # Finding out the max number of sequences we can generate of the given length\n",
    "  num_length = len(int_songs) - seq_len\n",
    "  inputs = []\n",
    "  targets = []\n",
    "\n",
    "  for i in range(num_length):\n",
    "    # the input sequence will be the sliced list of length 64\n",
    "    inputs.append(int_songs[i:i + seq_len])\n",
    "    # the output will be the element immediately succeeding the input sequence\n",
    "    targets.append(int_songs[i+seq_len])\n",
    "\n",
    "  # one hot encoding the sequences\n",
    "  vocab_size = len(set(int_songs))\n",
    "  inputs = keras.utils.to_categorical(inputs, num_classes = vocab_size)\n",
    "  targets = np.array(targets)\n",
    "\n",
    "  return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVwQUA2UgCEn"
   },
   "source": [
    "# Training a LSTM network for melody generation\n",
    "\n",
    "\n",
    "1. `build_model(output_units, num_units, loss, learning_rate)`: This function constructs the architecture for an LSTM-based neural network used in music generation. It sets up an input layer, adds an LSTM layer with a specified number of units, incorporates dropout to prevent overfitting, and concludes with an output layer using a softmax activation. The model is then compiled with the provided loss function, an Adam optimizer with the specified learning rate, and accuracy as a metric. Finally, it displays a summary of the model's structure and returns the compiled model.\n",
    "\n",
    "2. `train(output_units, num_units, loss, learning_rate)`: This function handles the training of the music generation model. It defines predetermined values for output units, loss, learning rate, and the number of units in the LSTM layer. It generates training sequences, likely comprising input and target sequences. The model is built using the `build_model` function, and training occurs with a specified number of epochs and batch size. Once trained, the model is saved to a file named \"model.h5\" in the \"/content/\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "AFg_BQ10hwBS",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.596729782Z",
     "start_time": "2023-11-10T10:36:19.534631236Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "BuUmd3xOgXbC",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:19.596778721Z",
     "start_time": "2023-11-10T10:36:19.534732519Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(output_units, num_units, loss, learning_rate):\n",
    "\n",
    "  # create the model architecture, we will be using keras functional API\n",
    "  input = keras.layers.Input(shape=(None, output_units))\n",
    "  x = keras.layers.LSTM(num_units[0])(input) #adding an LSTM layer\n",
    "  x = keras.layers.Dropout(0.2)(x) # adding a Dropout layer to avoid overfitting\n",
    "\n",
    "  output = keras.layers.Dense(output_units, activation=\"softmax\")(x)\n",
    "\n",
    "  model = keras.Model(input, output)\n",
    "\n",
    "  # compile model\n",
    "  model.compile(loss=loss, optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "  return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "hbS0WFccf0aV",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:40:10.624977979Z",
     "start_time": "2023-11-10T10:40:10.612444190Z"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  output_units = 23 # the vocabulary size\n",
    "  loss = \"sparse_categorical_crossentropy\"\n",
    "  learning_rate = 0.001\n",
    "  num_units = [256] # 1 layer with 256 neurons\n",
    "\n",
    "  # generate the training sequences\n",
    "  inputs, targets = generate_train()\n",
    "\n",
    "  # build the model\n",
    "  model = build_model(output_units, num_units, loss, learning_rate)\n",
    "\n",
    "  # train the model\n",
    "  model.fit(inputs, targets, epochs=50, batch_size=64)\n",
    "\n",
    "  # save the model\n",
    "  model.save(\"/home/deepakachu/Desktop/DMS/project/content/model1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9Bz6MSD3VGc"
   },
   "source": [
    " ## Generation of Melodies and Saving them\n",
    " \n",
    "The MelodyGen class is designed to create musical melodies using a pre-trained neural network model. It loads the model and a symbol-to-integer mapping dictionary during initialization. The generate method generates melodies from a given seed sequence, controlling the length and randomness. Temperature-based sampling influences the level of randomness. The melodies are constructed symbol by symbol, considering a maximum sequence length and an end-of-melody symbol ('/').\n",
    "\n",
    "The save_melody method converts the generated symbols into musical notation (MIDI format) and saves them to a file. This class offers flexibility in melody generation, allowing users to specify the number of steps in the melody and the level of randomness. Ultimately, it enables the creation and storage of musical compositions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "jiE0DwtN3Xio",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:40:12.787328849Z",
     "start_time": "2023-11-10T10:40:12.768594385Z"
    }
   },
   "outputs": [],
   "source": [
    "class MelodyGen:\n",
    "  def __init__(self, model_path = \"/home/deepakachu/Desktop/DMS/project/content/model1.h5\"):\n",
    "    self.model_path = model_path\n",
    "    self.model = keras.models.load_model(model_path)\n",
    "\n",
    "    with open(\"/home/deepakachu/Desktop/DMS/project/content/dataset1/mapping.json\", \"r\") as fp:\n",
    "      self._mappings = json.load(fp)\n",
    "\n",
    "    self._start_symbols = [\"/\"]*64\n",
    "\n",
    "  def generate(self, seed, num_steps, max_seq_len, temperature):\n",
    "    # create seed with symbols\n",
    "    seed = seed.split()\n",
    "    melody = seed\n",
    "    seed = self._start_symbols + seed\n",
    "    # map seed to int\n",
    "    # we are creating a seed and then mapping all the symbols\n",
    "    # to the integers using the mapping dict\n",
    "    seed = [self._mappings[symbol] for symbol in seed]\n",
    "\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "      # limit the seed to max_sequence_len\n",
    "      seed = seed[-max_seq_len:]\n",
    "      # one hot encode the seed\n",
    "      one_hot_seed = keras.utils.to_categorical(seed, num_classes=len(self._mappings))\n",
    "      # the below line just adds an extra axis to convert the array into 3d\n",
    "      one_hot_seed = one_hot_seed[np.newaxis, ...]\n",
    "\n",
    "      # making a prediction\n",
    "      probabilities = self.model.predict(one_hot_seed)[0]\n",
    "      output = self._sample_with_temperature(probabilities, temperature)\n",
    "      # update the seed so that we can feed it into network again\n",
    "      seed.append(output)\n",
    "\n",
    "      output_symbol = [k for k, v in self._mappings.items() if v == output][0]\n",
    "\n",
    "      # check whether we're at the end of a melody\n",
    "      if output_symbol == \"/\":\n",
    "        continue\n",
    "      # if not, append the predicted note\n",
    "      melody.append(output_symbol)\n",
    "    return melody\n",
    "\n",
    "  def _sample_with_temperature(self, probabilities, temperature):\n",
    "    \"\"\"\n",
    "    temp --> inf\n",
    "    this will make the sampling much more random, not specific/rigid\n",
    "    temp --> o\n",
    "    this will make the sampling more specific/rigid\n",
    "    \"\"\"\n",
    "    predictions = np.log(probabilities) / temperature\n",
    "    probabilities = np.exp(predictions) / np.sum(np.exp(predictions))\n",
    "\n",
    "    choices = range(len(probabilities)) # returns a list of choice indices\n",
    "    index = np.random.choice(choices, p=probabilities) # we sample one randomly\n",
    "\n",
    "    return index\n",
    "\n",
    "  def save_melody(self, melody, mood):\n",
    "    \"\"\"Converts a melody into a MIDI file\n",
    "  \n",
    "    :param melody (list of str):\n",
    "    :param min_duration (float): Duration of each time step in quarter length\n",
    "    :param file_name (str): Name of midi file\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    step_duration = 0\n",
    "    \n",
    "    if mood == \"h\":\n",
    "      step_duration = 0.5\n",
    "    elif mood == \"s\":\n",
    "      step_duration = 1\n",
    "    else:\n",
    "      step_duration = 0.1\n",
    "    # create a music21 stream\n",
    "    stream = m21.stream.Stream()\n",
    "  \n",
    "    start_symbol = None\n",
    "    step_counter = 1\n",
    "  \n",
    "    # parse all the symbols in the melody and create note/rest objects\n",
    "    for i, symbol in enumerate(melody):\n",
    "  \n",
    "      # handle case in which we have a note/rest\n",
    "      if symbol != \"_\" or i + 1 == len(melody):\n",
    "  \n",
    "        # ensure we're dealing with note/rest beyond the first one\n",
    "        if start_symbol is not None:\n",
    "  \n",
    "          quarter_length_duration = step_duration * step_counter # 0.25 * 4 = 1\n",
    "  \n",
    "          # handle rest\n",
    "          if start_symbol == \"r\":\n",
    "            m21_event = m21.note.Rest(quarterLength=quarter_length_duration)\n",
    "  \n",
    "          # handle note\n",
    "          else:\n",
    "            m21_event = m21.note.Note(int(start_symbol), quarterLength=quarter_length_duration)\n",
    "  \n",
    "          stream.append(m21_event)\n",
    "  \n",
    "          # reset the step counter\n",
    "          step_counter = 1\n",
    "  \n",
    "        start_symbol = symbol\n",
    "  \n",
    "      # handle case in which we have a prolongation sign \"_\"\n",
    "      else:\n",
    "        step_counter += 1\n",
    "  \n",
    "    # write the m21 stream to a midi file\n",
    "    stream.write('midi', fp='/home/deepakachu/Desktop/DMS/project/content/meow.mid')\n",
    "    # stream.write(format, file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "2tOjmf6hhM3A",
    "outputId": "ebcb74fd-c606-4cc3-ecf0-cfedb7f55ba8",
    "ExecuteTime": {
     "end_time": "2023-11-10T10:41:08.651165940Z",
     "start_time": "2023-11-10T10:41:07.585761448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading songs...\n",
      "Loaded 12 songs.\n",
      "e minor\n",
      "C major\n",
      "e minor\n",
      "e minor\n",
      "F major\n",
      "C major\n",
      "C major\n",
      "F major\n",
      "F major\n",
      "g minor\n",
      "e minor\n",
      "b minor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, None, 23)]        0         \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 256)               286720    \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 23)                5911      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 292631 (1.12 MB)\n",
      "Trainable params: 292631 (1.12 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 16:11:08.611912: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:447] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2023-11-10 16:11:08.611973: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:451] Memory usage: 6488064 bytes free, 4100784128 bytes total.\n",
      "2023-11-10 16:11:08.611984: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at cudnn_rnn_ops.cc:1764 : UNKNOWN: Fail to find the dnn implementation.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node CudnnRNN defined at (most recent call last):\n<stack traces unavailable>\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[model_11/lstm_11/PartitionedCall]] [Op:__inference_train_function_12459]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnknownError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[71], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m songs \u001B[38;5;241m=\u001B[39m single_file(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/home/deepakachu/Desktop/DMS/project/content/dataset1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/home/deepakachu/Desktop/DMS/project/content/dataset1/single_file\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m mapping(songs)\n\u001B[0;32m----> 6\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m happy \u001B[38;5;241m=\u001B[39m [ \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m69 _ 68 _ 72 _ 64 _ 62 _ 74 _ 60\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m55 _ 57 _ 60 _ 62 _ 65 _ 67 _ 71\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m72 _ 74 _ 76 _ 60 _ 64 _ 67 _ 69\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m65 _ 67 _ 71 _ 74 _ 76 _ 60 _ 64\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m67 _ 71 _ 74 _ 76 _ 60 _ 64 _ 55\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      8\u001B[0m sad \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m77 _ _ 76 _ _ 67 _ _ 65 _ _ 71 _ _ 55 _ _ 57\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m69 _ _ 68 _ _ 72 _ _ 64 _ _ 62 _ _ 74 _ _ 60\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m64 _ _ 55 _ _ 69 _ _ 72 _ _ 81 _ _ 76 _ _ 60\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m60 _ _ 65 _ _ 57 _ _ 68 _ _ 74 _ _ 72 _ _ 81\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m76 _ _ 67 _ _ 60 _ _ 65 _ _ 74 _ _ 72 _ _ 81\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "Cell \u001B[0;32mIn[68], line 14\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m build_model(output_units, num_units, loss, learning_rate)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# train the model\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# save the model\u001B[39;00m\n\u001B[1;32m     17\u001B[0m model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/home/deepakachu/Desktop/DMS/project/content/model1.h5\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/GG_1110/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/anaconda3/envs/GG_1110/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:60\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     53\u001B[0m   \u001B[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001B[39;00m\n\u001B[1;32m     54\u001B[0m   inputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     55\u001B[0m       tensor_conversion_registry\u001B[38;5;241m.\u001B[39mconvert(t)\n\u001B[1;32m     56\u001B[0m       \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, core_types\u001B[38;5;241m.\u001B[39mTensor)\n\u001B[1;32m     57\u001B[0m       \u001B[38;5;28;01melse\u001B[39;00m t\n\u001B[1;32m     58\u001B[0m       \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m inputs\n\u001B[1;32m     59\u001B[0m   ]\n\u001B[0;32m---> 60\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[1;32m     61\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     63\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mUnknownError\u001B[0m: Graph execution error:\n\nDetected at node CudnnRNN defined at (most recent call last):\n<stack traces unavailable>\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[model_11/lstm_11/PartitionedCall]] [Op:__inference_train_function_12459]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  import random\n",
    "  preprocess(\"/home/deepakachu/Desktop/DMS/project/content/essen/europa/deutschl/test\")\n",
    "  songs = single_file(\"/home/deepakachu/Desktop/DMS/project/content/dataset1\", \"/home/deepakachu/Desktop/DMS/project/content/dataset1/single_file\")\n",
    "  mapping(songs)\n",
    "  train()\n",
    "  happy = [ \"69 _ 68 _ 72 _ 64 _ 62 _ 74 _ 60\", \"55 _ 57 _ 60 _ 62 _ 65 _ 67 _ 71\", \"72 _ 74 _ 76 _ 60 _ 64 _ 67 _ 69\", \"65 _ 67 _ 71 _ 74 _ 76 _ 60 _ 64\", \"67 _ 71 _ 74 _ 76 _ 60 _ 64 _ 55\"]\n",
    "  sad = [\"77 _ _ 76 _ _ 67 _ _ 65 _ _ 71 _ _ 55 _ _ 57\", \"69 _ _ 68 _ _ 72 _ _ 64 _ _ 62 _ _ 74 _ _ 60\", \"64 _ _ 55 _ _ 69 _ _ 72 _ _ 81 _ _ 76 _ _ 60\", \"60 _ _ 65 _ _ 57 _ _ 68 _ _ 74 _ _ 72 _ _ 81\", \"76 _ _ 67 _ _ 60 _ _ 65 _ _ 74 _ _ 72 _ _ 81\"]\n",
    "  angry = [\"77 _ 76 _ 67 _ 65 _ 71  _ 55 _ 57\", \"69 _ 68 _ 72 _ 64 _ 62 _ 74 _ 60\", \"64 _ 55 _ 69 _ 72 _ 81 _ 76  _ 60\", \"60 _ 65 _ 57 _ 68 _ 74 _ 72 _ 81\", \"76 _ 67 _ 60 _ 65 _ 74 _ 72 _ 81\"]\n",
    "  mood = input(\"enter moood: \")\n",
    "  mg = MelodyGen()\n",
    "  seed = \"\"\n",
    "  if mood == \"s\":\n",
    "    seed = random.choice(sad)\n",
    "  elif mood == \"h\":\n",
    "    seed = random.choice(happy)\n",
    "  else:\n",
    "    seed = random.choice(angry)\n",
    "  melody = mg.generate(seed, 500, 64, 50)\n",
    "  mg.save_melody(melody, mood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:20.186127057Z",
     "start_time": "2023-11-10T10:36:20.186058262Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Assuming you have already created your model using build_model or another method\n",
    "\n",
    "# Visualize the model architecture\n",
    "plot_model(mg.model, to_file='/home/deepakachu/Desktop/DMS/project/content/model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Get the model summary\n",
    "# mg.model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T10:36:20.230358456Z",
     "start_time": "2023-11-10T10:36:20.230215292Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
